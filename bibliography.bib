
@book{bianchi2017,
  title = {Recurrent {{Neural Networks}} for {{Short}}-{{Term Load Forecasting}}: {{An Overview}} and {{Comparative Analysis}}},
  shorttitle = {Recurrent {{Neural Networks}} for {{Short}}-{{Term Load Forecasting}}},
  author = {Bianchi, Filippo Maria and Jenssen, Robert and Kampffmeyer, Michael C. and Maiorino, Enrico and Rizzi, Antonello},
  date = {2017},
  edition = {1st ed. 2017},
  publisher = {{Springer International Publishing : Imprint: Springer}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-70338-1},
  abstract = {The key component in forecasting demand and consumption of resources in a supply network is an accurate prediction of real-valued time series. Indeed, both service interruptions and resource waste can be reduced with the implementation of an effective forecasting system. Significant research has thus been devoted to the design and development of methodologies for short term load forecasting over the past decades. A class of mathematical models, called Recurrent Neural Networks, are nowadays gaining renewed interest among researchers and they are replacing many practical implementations of the forecasting systems, previously based on static methods. Despite the undeniable expressive power of these architectures, their recurrent nature complicates their understanding and poses challenges in the training procedures. Recently, new important families of recurrent architectures have emerged and their applicability in the context of load forecasting has not been investigated completely yet. This work performs a comparative study on the problem of Short-Term Load Forecast, by using different classes of state-of-the-art Recurrent Neural Networks. The authors test the reviewed models first on controlled synthetic tasks and then on different real datasets, covering important practical cases of study. The text also provides a general overview of the most important architectures and defines guidelines for configuring the recurrent networks to predict real-valued time series},
  file = {/Users/romandusek/Zotero/storage/EM2KEN6A/Bianchi et al. - 2017 - Recurrent Neural Networks for Short-Term Load Fore.pdf},
  isbn = {978-3-319-70338-1},
  keywords = {Artificial intelligence,Artificial Intelligence,Computer software-Reusability,Computer system failures,Energy efficiency,Energy Efficiency,Performance and Reliability,Power electronics,Power Electronics; Electrical Machines and Networks,System Performance and Evaluation},
  langid = {english},
  pagetotal = {1},
  series = {{{SpringerBriefs}} in {{Computer Science}}}
}

@online{britz2015,
  title = {Recurrent {{Neural Networks Tutorial}}, {{Part}} 1 – {{Introduction}} to {{RNNs}}},
  author = {Britz, Denny},
  date = {2015-09-17T11:28:23+00:00},
  url = {http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/},
  urldate = {2021-02-03},
  abstract = {Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many~NLP tasks. But despite their recent popularity I’ve only found a limited number of resources that thr…},
  file = {/Users/romandusek/Zotero/storage/NI48VAEC/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns.html},
  keywords = {read},
  langid = {american},
  organization = {{WildML}}
}

@online{britz2015a,
  title = {Recurrent {{Neural Networks Tutorial}}, {{Part}} 2 – {{Implementing}} a {{RNN}} with {{Python}}, {{Numpy}} and {{Theano}}},
  author = {Britz, Denny},
  date = {2015-09-30T13:19:19+00:00},
  url = {http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/},
  urldate = {2021-02-11},
  abstract = {This the second part of the Recurrent Neural Network Tutorial. The first part is here. Code to follow along is on Github. In this part we will implement a full Recurrent Neural Network from scratch…},
  file = {/Users/romandusek/Zotero/storage/KYY57R4W/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-a.html},
  keywords = {read},
  langid = {american},
  organization = {{WildML}}
}

@online{britz2015b,
  title = {Recurrent {{Neural Networks Tutorial}}, {{Part}} 3 – {{Backpropagation Through Time}} and {{Vanishing Gradients}}},
  author = {Britz, Denny},
  date = {2015-10-08T12:00:10+00:00},
  url = {http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/},
  urldate = {2021-02-11},
  abstract = {This the third~part of the Recurrent Neural Network Tutorial. In the previous~part of the tutorial we implemented a RNN from scratch, but didn’t go into detail on how Backpropagation Through …},
  file = {/Users/romandusek/Zotero/storage/4Z3F5P6M/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients.html},
  keywords = {read},
  langid = {american},
  organization = {{WildML}}
}

@online{britz2015c,
  title = {Recurrent {{Neural Network Tutorial}}, {{Part}} 4 – {{Implementing}} a {{GRU}}/{{LSTM RNN}} with {{Python}} and {{Theano}}},
  author = {Britz, Denny},
  date = {2015-10-27T08:47:27+00:00},
  url = {http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/},
  urldate = {2021-02-11},
  abstract = {The code for this post is on Github. This is~part 4, the last part of the~Recurrent Neural Network Tutorial. The previous parts~are: Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNN…},
  file = {/Users/romandusek/Zotero/storage/7IT9U4D6/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano.html},
  keywords = {read},
  langid = {american},
  organization = {{WildML}}
}

@thesis{bukovsky2012,
  title = {Nonconventional Neural Architectures and their Advantages for Technical Applications},
  author = {Bukovský, Ivo},
  date = {2012},
  institution = {{Faculty of Mechanical Engineering, Czech Technical University in Prague}},
  url = {http://users.fs.cvut.cz/ivo.bukovsky/publications/Teze_IB_86_bw_1200dpi.pdf},
  file = {/Users/romandusek/Zotero/storage/938SZ56T/Bukovský - ČESKÉ VYSOKÉ UČENÍ TECHNICKÉ V PRAZE FAKULTA STROJ.pdf},
  keywords = {read},
  langid = {czech},
  type = {Habilitation thesis}
}

@book{desker,
  title = {Recurrent {{Neural Netowrks Design}} and {{Applications}}},
  author = {Desker, L.R. and Jain, L.C.},
  file = {/Users/romandusek/Zotero/storage/GBBKKA9V/Recurrent Neural Networks Design And Applications.pdf}
}

@online{flovik2018,
  title = {How (Not) to Use {{Machine Learning}} for Time Series Forecasting: {{The}} Sequel | {{LinkedIn}}},
  author = {Flovik, Vegard},
  year = {4, 2018},
  url = {https://www.linkedin.com/pulse/how-use-machine-learning-time-series-forecasting-vegard-flovik-phd-1f/},
  urldate = {2021-02-11}
}

@online{flovik2019,
  title = {How (Not) to Use {{Machine Learning}} for Time Series Forecasting: {{Avoiding}} the Pitfalls | {{LinkedIn}}},
  author = {Flovik, Vegard},
  year = {12, 2019},
  url = {https://www.linkedin.com/pulse/how-use-machine-learning-time-series-forecasting-vegard-flovik-phd/},
  urldate = {2021-02-11}
}

@book{goodfellow2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {{MIT Press}},
  url = {http://www.deeplearningbook.org},
  file = {/Users/romandusek/Zotero/storage/LVUUAY3W/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf}
}

@book{gupta2003,
  title = {Static {{And Dynamic Neural Networks}} - {{Funds}} to {{Adv}}. {{Theory}} by {{M}}. {{Gupta}}, {{L}}. {{Jin}}, {{N}}. {{Homma}}},
  author = {Gupta, M. Madan and Homma, N. and Jin, L.},
  date = {2003},
  publisher = {{John Wiley \& Sons, Ltd}},
  file = {/Users/romandusek/Zotero/storage/CD4FAKKH/Static And Dynamic Neural Networks - Funds to Adv. Theory by M. Gupta, L. Jin, N. Homma (z-lib.org).pdf}
}

@incollection{gupta2012,
  title = {Fundamentals of {{Higher Order Neural Networks}} for {{Modeling}} and {{Simulation}}},
  booktitle = {Fundamentals of {{Higher Order Neural Networks}} for {{Modeling}} and {{Simulation}}},
  author = {Gupta, M. Madan and Bukovsky, Ivo},
  date = {2012},
  publisher = {{IGI Global}},
  file = {/Users/romandusek/Zotero/storage/V9E6HXCD/Gupta M. Madan a Bukovsky, Ivo - Fundamentals of Higher Order Neural Networks for M.pdf},
  keywords = {read}
}

@article{kosmatopoulos1995,
  title = {High-Order Neural Network Structures for Identification of Dynamical Systems},
  author = {Kosmatopoulos, E. B. and Polycarpou, M. M. and Christodoulou, M. A. and Ioannou, P. A.},
  date = {1995-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {6},
  pages = {422--431},
  issn = {1941-0093},
  doi = {10.1109/72.363477},
  abstract = {Several continuous-time and discrete-time recurrent neural network models have been developed and applied to various engineering problems. One of the difficulties encountered in the application of recurrent networks is the derivation of efficient learning algorithms that also guarantee the stability of the overall system. This paper studies the approximation and learning properties of one class of recurrent networks, known as high-order neural networks; and applies these architectures to the identification of dynamical systems. In recurrent high-order neural networks, the dynamic components are distributed throughout the network in the form of dynamic neurons. It is shown that if enough high-order connections are allowed then this network is capable of approximating arbitrary dynamical systems. Identification schemes based on high-order network architectures are designed and analyzed.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  file = {/Users/romandusek/Zotero/storage/Z82XAGK9/363477.html},
  keywords = {Algorithm design and analysis,approximation properties,continuous-time recurrent neural network models,discrete-time recurrent neural network models,dynamic neurons,dynamical systems identification,efficient learning algorithms,engineering problems,Feedforward neural networks,high-order connections,high-order neural network structures,identification,learning (artificial intelligence),learning properties,Multi-layer neural network,neural net architecture,Neural networks,Neurofeedback,Neurons,overall system stability,recurrent neural nets,Recurrent neural networks,stability,Stability analysis,Transfer functions},
  number = {2}
}

@book{lazyprogrammer,
  title = {Recurrent Neural Networks in {{Python}}},
  author = {LazyProgrammer},
  file = {/Users/romandusek/Zotero/storage/9NTXGSUX/Deep Learning Recurrent Neural Networks in Python by LazyProgrammer.pdf}
}

@article{mandic,
  title = {Recurrent Neural Networks for Prediction Learning Algorithms, Architectures, and Stability by {{Danilo Mandic}}, {{Jonathon Chambers}}},
  author = {Mandic, Danilo and Chambers, Jonathon A.},
  file = {/Users/romandusek/Zotero/storage/RKVJEG6Y/Recurrent neural networks for prediction learning algorithms, architectures, and stability by Danilo Mandic, Jonathon Chambers (z-lib.org).djvu}
}

@book{mandic2001,
  title = {Recurrent {{Neural Networks}} for {{Prediction}}},
  author = {Mandic, Danilo P. and Chambers, Jonathon A.},
  date = {2001-08-06},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/047084535X},
  url = {http://doi.wiley.com/10.1002/047084535X},
  urldate = {2021-01-27},
  editorb = {Haykin, Simon},
  editorbtype = {redactor},
  file = {/Users/romandusek/Zotero/storage/JGZJTJCU/Mandic a Chambers - 2001 - Recurrent Neural Networks for Prediction.pdf},
  isbn = {978-0-471-49517-8 978-0-470-84535-6},
  langid = {english},
  series = {Wiley {{Series}} in {{Adaptive}} and {{Learning Systems}} for {{Signal Processing}}, {{Communications}}, and {{Control}}}
}

@online{olah2015,
  title = {Understanding {{LSTM Networks}}},
  author = {Olah, Christopher},
  date = {2015-08-27},
  url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2021-02-01},
  langid = {english}
}

@book{rios2019,
  title = {Neural Networks Modeling and Control: Applications for Unknown Nonlinear Delayed Systems in Discrete Time},
  shorttitle = {Neural Networks Modeling and Control},
  author = {Rios, Jorge D. and Alanis, Alma Y. and Arana-Daniel, Nancy and Lopez-Franco, Carlos and Sanchez, Edgar N.},
  date = {2019},
  edition = {1},
  publisher = {{Elsevier}},
  location = {{Waltham}},
  abstract = {"Edited by Alma Y. Alanis, University of Guadalajara Guadalajara, Jalisco, Mexico; Jorge Rios; Nancy Arana-Daniel, University of Guadalajara, Guadalajara, Jalisco, Mexico and Carlos Lopez-Franco, University of Guadalajara, Guadalajara, Jalisco, Mexico"--},
  file = {/Users/romandusek/Zotero/storage/7PLSN7GI/Rios et al. - 2019 - Neural networks modeling and control applications.pdf},
  isbn = {978-0-12-817078-6},
  langid = {english}
}

@book{rovithakis2000,
  title = {Adaptive {{Control}} with {{Recurrent High}}-Order {{Neural Networks}}},
  author = {Rovithakis, George A. and Christodoulou, Manolis A.},
  date = {2000},
  publisher = {{Springer London}},
  location = {{London}},
  doi = {10.1007/978-1-4471-0785-9},
  url = {http://link.springer.com/10.1007/978-1-4471-0785-9},
  urldate = {2021-02-02},
  editorb = {Grimble, Michael J. and Johnson, Michael A.},
  editorbtype = {redactor},
  file = {/Users/romandusek/Zotero/storage/MXC366TA/Rovithakis a Christodoulou - 2000 - Adaptive Control with Recurrent High-order Neural .pdf},
  isbn = {978-1-4471-1201-3 978-1-4471-0785-9},
  keywords = {read},
  langid = {english},
  pagetotal = {2-29},
  series = {Advances in {{Industrial Control}}}
}

@book{sanchez2008,
  title = {Discrete-{{Time High Order Neural Control}}: {{Trained}} with {{Kaiman Filtering}}},
  shorttitle = {Discrete-{{Time High Order Neural Control}}},
  author = {Sanchez, Edgar N. and Alanís, Alma Y. and Loukianov, Alexander G.},
  date = {2008},
  volume = {112},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78289-6},
  url = {http://link.springer.com/10.1007/978-3-540-78289-6},
  urldate = {2021-02-02},
  editorb = {Kacprzyk, Janusz},
  editorbtype = {redactor},
  file = {/Users/romandusek/Zotero/storage/KVTMEMCR/Sanchez et al. - 2008 - Discrete-Time High Order Neural Control Trained w.pdf},
  isbn = {978-3-540-78288-9 978-3-540-78289-6},
  keywords = {read},
  langid = {english},
  pagetotal = {7, 11-18, 29-31},
  series = {Studies in {{Computational Intelligence}}}
}

@inproceedings{shin1991,
  title = {The Pi-Sigma Network: An Efficient Higher-Order Neural Network for Pattern Classification and Function Approximation},
  shorttitle = {The Pi-Sigma Network},
  booktitle = {{{IJCNN}}-91-{{Seattle International Joint Conference}} on {{Neural Networks}}},
  author = {Shin, Y. and Ghosh, J.},
  date = {1991-07},
  volume = {i},
  pages = {13-18 vol.1},
  doi = {10.1109/IJCNN.1991.155142},
  abstract = {Introduces a novel feedforward network called the pi-sigma network. This network utilizes product cells as the output units to indirectly incorporate the capabilities of higher-order networks while using a fewer number of weights and processing units. The network has a regular structure, exhibits much faster learning, and is amenable to the incremental addition of units to attain a desired level of complexity. Simulation results show good convergence properties and accuracy for function approximation. Comparative results using the DARPA acoustic transient data set are also provided to highlight the classification abilities of pi-sigma networks.{$<>$}},
  eventtitle = {{{IJCNN}}-91-{{Seattle International Joint Conference}} on {{Neural Networks}}},
  file = {/Users/romandusek/Zotero/storage/VZDAMCY7/155142.html},
  keywords = {accuracy,Backpropagation,complexity,Computer networks,computerised pattern recognition,convergence,Convergence,convergence properties,DARPA acoustic transient data set,feedforward network,function approximation,Function approximation,higher-order neural network,learning,Logic,Multilayer perceptrons,neural nets,Neural networks,Noise measurement,pattern classification,Pattern classification,pi-sigma network,Polynomials,processing units,product cells,simulation,sonar,weights}
}

@article{taylor1993,
  title = {Learning Higher Order Correlations},
  author = {Taylor, J. G. and Coombes, S.},
  date = {1993-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {6},
  pages = {423--427},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(93)90009-L},
  url = {http://www.sciencedirect.com/science/article/pii/089360809390009L},
  urldate = {2021-02-02},
  abstract = {We present an extension of Oja's learning algorithm for principal component analysis which is capable of adapting the weights of a higher order neuron to pick up higher order correlations from a given data set. The output of such a neuron is recognised as a decision hypersurface in the data space and as such the generalised Oja neuron may be used for pattern classification. The generalised Oja neuron is also shown to be capable of fitting hypersurfaces optimally to a data set.},
  file = {/Users/romandusek/Zotero/storage/CL7YUR6J/Taylor a Coombes - 1993 - Learning higher order correlations.pdf},
  keywords = {Higher order neurons,Hypersurface fitting,Pattern classification,Principal component analysis},
  langid = {english},
  number = {3}
}

@article{widrow1990,
  title = {30 Years of Adaptive Neural Networks: Perceptron, {{Madaline}}, and Backpropagation},
  shorttitle = {30 Years of Adaptive Neural Networks},
  author = {Widrow, B. and Lehr, M.A.},
  year = {Sept./1990},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {78},
  pages = {1415--1442},
  issn = {00189219},
  doi = {10.1109/5.58323},
  url = {http://ieeexplore.ieee.org/document/58323/},
  urldate = {2021-02-11},
  file = {/Users/romandusek/Zotero/storage/AJQRPZUK/Widrow a Lehr - 1990 - 30 years of adaptive neural networks perceptron, .pdf},
  langid = {english},
  number = {9}
}

@book{zhang2008,
  title = {Artificial {{Higher Order Neural Networks}} for {{Economics}} and {{Business}}},
  author = {Zhang, Ming},
  date = {2008},
  publisher = {{IGI Global}},
  abstract = {This book is the first book to provide opportunities for millions working in economics, accounting, finance and other business areas education on HONNs, the ease of their usage, and directions on how to obtain more accurate application results. It provides significant, informative advancements in the subject and introduces the HONN group models and adaptive HONNs”--Provided by publisher.},
  file = {/Users/romandusek/Zotero/storage/X3879GC3/Zhang - Artificial Higher Order Neural Networks for Econom.pdf},
  isbn = {978-1-59904-897-0},
  keywords = {read},
  langid = {english}
}


